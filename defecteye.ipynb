{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training YOLOv8 Model for 3D Printer Error Detection\n",
    "\n",
    "This notebook documents the process of training a YOLOv8 model to detect errors in 3D printers using a labeled dataset. The steps include:\n",
    "1. Installing required libraries.\n",
    "2. Loading the YOLOv8 model.\n",
    "3. Setting up the dataset paths.\n",
    "4. Training the model.\n",
    "5. Evaluating the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Install Required Libraries\n",
    "Install the necessary Python libraries, including YOLOv8, PyTorch, and OpenCV, for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: ultralytics in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (8.3.70)\n",
      "Requirement already satisfied: torch in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (0.21.0+cu126)\n",
      "Requirement already satisfied: opencv-python-headless in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (2.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (1.15.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (6.1.1)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: filelock in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: colorama in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in d:\\coding projects\\defecteye\\pyvenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install YOLOv8 and PyTorch dependencies\n",
    "%pip install ultralytics torch torchvision opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dependencies\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Force GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"No GPU found!\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load YOLOv8 Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11m.pt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yaml_path = \"D:/Coding Projects/DefectEye/Data/data.yaml\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fine-Tuning YOLOV8 m Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=D:/Coding Projects/DefectEye/Data/data.yaml, epochs=60, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=defecteye_finetuned3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=d:\\Coding Projects\\DefectEye\\runs\\detect\\defecteye_finetuned3\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "YOLO11m summary: 409 layers, 20,055,321 parameters, 20,055,305 gradients, 68.2 GFLOPs\n",
      "\n",
      "Transferred 643/649 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:00<00:00, 10.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Coding Projects\\DefectEye\\Data\\train\\labels... 2757 images, 6 backgrounds, 0 corrupt: 100%|██████████| 2757/2757 [00:02<00:00, 1237.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Coding Projects\\DefectEye\\Data\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Coding Projects\\DefectEye\\Data\\valid\\labels... 167 images, 0 backgrounds, 0 corrupt: 100%|██████████| 167/167 [00:00<00:00, 1017.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\Coding Projects\\DefectEye\\Data\\valid\\labels.cache\n",
      "Plotting labels to d:\\Coding Projects\\DefectEye\\runs\\detect\\defecteye_finetuned3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1md:\\Coding Projects\\DefectEye\\runs\\detect\\defecteye_finetuned3\u001b[0m\n",
      "Starting training for 60 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/60      1.33G      2.051      2.937      2.017          4        640:  66%|██████▌   | 452/690 [00:20<00:10, 21.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_yaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to dataset YAML file\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Input image size\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Number of fine-tuning epochs\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m#cannot go higher than 16, due to hardware constraints!\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdefecteye_finetuned\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Force GPU usage\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Freeze the first 10 layers for transfer learning\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:808\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:380\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m    379\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:109\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:290\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[1;34m(self, batch, preds)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m--> 290\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(preds, batch)\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:110\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:149\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 149\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    150\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:1042\u001b[0m, in \u001b[0;36mC2PSA.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes the input tensor 'x' through a series of PSA blocks and returns the transformed tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m a, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1042\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat((a, b), \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:957\u001b[0m, in \u001b[0;36mPSABlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes a forward pass through PSABlock, applying attention and feed-forward layers to the input tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m    956\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x)\n\u001b[1;32m--> 957\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x)\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:51\u001b[0m, in \u001b[0;36mConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:432\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding Projects\\DefectEye\\pyvenv\\Lib\\site-packages\\torch\\nn\\functional.py:2379\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = model.train(\n",
    "    data=data_yaml_path,  # Path to dataset YAML file\n",
    "    imgsz=640,            # Input image size\n",
    "    epochs=60,            # Number of fine-tuning epochs\n",
    "    batch=4,               #cannot go higher than 16, due to hardware constraints!\n",
    "    name='defecteye_finetuned', \n",
    "    device=device,        # Force GPU usage\n",
    "    freeze=10,             # Freeze the first 10 layers for transfer learning\n",
    "    amp=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.70  Python-3.11.9 torch-2.6.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4070 Ti, 12282MiB)\n",
      "Model summary (fused): 218 layers, 25,840,339 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Coding Projects\\DefectEye\\Data\\valid\\labels.cache... 48 images, 0 backgrounds, 0 corrupt: 100%|██████████| 48/48 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         48         82      0.508      0.366       0.35      0.154\n",
      "Speed: 0.4ms preprocess, 11.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1md:\\Coding Projects\\DefectEye\\runs\\detect\\defecteye_finetuned2\u001b[0m\n",
      "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([0])\n",
      "box: ultralytics.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000025B800DD910>\n",
      "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
      "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,\n",
      "            0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,\n",
      "            0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,\n",
      "            0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,\n",
      "            0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,\n",
      "            0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.91667,     0.88235,     0.88235,     0.88235,\n",
      "            0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,\n",
      "            0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.88235,     0.72727,\n",
      "            0.72727,     0.72727,     0.72727,     0.72727,     0.72727,     0.72727,     0.72727,     0.72727,     0.72727,     0.72727,     0.72727,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,\n",
      "            0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.69231,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,\n",
      "            0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.66667,     0.65625,     0.65625,     0.65625,     0.65625,     0.65625,     0.65625,     0.65625,     0.65625,     0.65625,\n",
      "            0.65625,     0.65625,     0.65625,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.62162,\n",
      "            0.62162,     0.62162,     0.62162,     0.62162,     0.62162,     0.61538,     0.61538,     0.61538,     0.61538,     0.61538,     0.61538,     0.61538,     0.61538,     0.61538,     0.61538,     0.61538,     0.61538,      0.5814,      0.5814,      0.5814,      0.5814,      0.5814,      0.5814,\n",
      "             0.5814,      0.5814,      0.5814,      0.5814,      0.5814,      0.5814,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,\n",
      "            0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,\n",
      "            0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.56863,     0.53571,     0.53571,     0.53571,     0.53571,     0.53571,     0.53571,     0.53571,     0.53571,     0.53571,     0.53571,     0.53571,     0.53571,      0.4697,      0.4697,\n",
      "             0.4697,      0.4697,      0.4697,      0.4697,      0.4697,      0.4697,      0.4697,      0.4697,      0.4697,      0.4697,      0.4507,      0.4507,      0.4507,      0.4507,      0.4507,      0.4507,      0.4507,      0.4507,      0.4507,      0.4507,      0.4507,      0.4507,     0.34021,\n",
      "            0.34021,     0.34021,     0.34021,     0.34021,     0.34021,     0.34021,     0.34021,     0.34021,     0.34021,     0.34021,     0.34021,     0.34021,     0.30909,     0.30909,     0.30909,     0.30909,     0.30909,     0.30909,     0.30909,     0.30909,     0.30909,     0.30909,     0.30909,\n",
      "            0.30909,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,     0.27692,\n",
      "            0.27692,     0.27692,     0.27612,     0.27612,     0.27612,     0.27612,     0.27612,     0.27612,     0.27612,     0.27612,     0.27612,     0.27612,     0.27612,     0.27612,     0.27143,     0.27143,     0.27143,     0.27143,     0.27143,     0.27143,     0.27143,     0.27143,     0.27143,\n",
      "            0.27143,     0.27143,     0.27143,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.24845,\n",
      "            0.24845,     0.24845,     0.24845,     0.24845,     0.24845,     0.23699,     0.23699,     0.23699,     0.23699,     0.23699,     0.23699,     0.23699,     0.23699,     0.23699,     0.23699,     0.23699,     0.23699,     0.21649,     0.21649,     0.21649,     0.21649,     0.21649,     0.21649,\n",
      "            0.21649,     0.21649,     0.21649,     0.21649,     0.21649,     0.21649,     0.19725,     0.19725,     0.19725,     0.19725,     0.19725,     0.19725,     0.19725,     0.19725,     0.19725,     0.19725,     0.19725,     0.19725,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,\n",
      "            0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.16667,     0.13731,     0.13731,     0.13731,\n",
      "            0.13731,     0.13731,     0.13731,     0.13731,     0.13731,     0.13731,     0.13731,     0.13731,     0.13731,    0.097107,    0.097107,    0.097107,    0.097107,    0.097107,    0.097107,    0.097107,    0.097107,    0.097107,    0.097107,    0.097107,    0.097107,    0.081494,    0.081494,\n",
      "           0.081494,    0.081494,    0.081494,    0.081494,    0.081494,    0.081494,    0.081494,    0.081494,    0.081494,    0.081494,    0.062548,    0.062397,    0.062246,    0.062095,    0.061944,    0.061793,    0.061642,    0.061491,     0.06134,    0.061189,    0.061037,    0.060886,    0.060735,\n",
      "           0.060584,    0.060433,    0.060282,    0.060131,     0.05998,    0.059829,    0.059678,    0.059527,    0.059376,    0.059224,    0.059073,    0.058922,    0.058771,     0.05862,    0.058469,    0.058318,    0.058167,    0.058016,    0.057865,    0.057714,    0.057563,    0.057412,     0.05726,\n",
      "           0.057109,    0.056958,    0.056807,    0.056656,    0.056505,    0.056354,    0.056203,    0.056052,    0.055901,     0.05575,    0.055599,    0.055447,    0.055296,    0.055145,    0.054994,    0.054843,    0.054692,    0.054541,     0.05439,    0.054239,    0.054088,    0.053937,    0.053786,\n",
      "           0.053634,    0.053483,    0.053332,    0.053181,     0.05303,    0.052879,    0.052728,    0.052577,    0.052426,    0.052275,    0.052124,    0.051973,    0.051821,     0.05167,    0.051519,    0.051368,    0.051217,    0.051066,    0.050915,    0.050764,    0.050613,    0.050462,    0.050311,\n",
      "            0.05016,    0.050008,    0.049857,    0.049706,    0.049555,    0.049404,    0.049253,    0.049102,    0.048951,      0.0488,    0.048649,    0.048498,    0.048347,    0.048195,    0.048044,    0.047893,    0.047742,    0.047591,     0.04744,    0.047289,    0.047138,    0.046987,    0.046836,\n",
      "           0.046685,    0.046534,    0.046382,    0.046231,     0.04608,    0.045929,    0.045778,    0.045627,    0.045476,    0.045325,    0.045174,    0.045023,    0.044872,    0.044721,    0.044569,    0.044418,    0.044267,    0.044116,    0.043965,    0.043814,    0.043663,    0.043512,    0.043361,\n",
      "            0.04321,    0.043059,    0.042908,    0.042756,    0.042605,    0.042454,    0.042303,    0.042152,    0.042001,     0.04185,    0.041699,    0.041548,    0.041397,    0.041246,    0.041095,    0.040943,    0.040792,    0.040641,     0.04049,    0.040339,    0.040188,    0.040037,    0.039886,\n",
      "           0.039735,    0.039584,    0.039433,    0.039282,     0.03913,    0.038979,    0.038828,    0.038677,    0.038526,    0.038375,    0.038224,    0.038073,    0.037922,    0.037771,     0.03762,    0.037469,    0.037317,    0.037166,    0.037015,    0.036864,    0.036713,    0.036562,    0.036411,\n",
      "            0.03626,    0.036109,    0.035958,    0.035807,    0.035656,    0.035504,    0.035353,    0.035202,    0.035051,      0.0349,    0.034749,    0.034598,    0.034447,    0.034296,    0.034145,    0.033994,    0.033843,    0.033691,     0.03354,    0.033389,    0.033238,    0.033087,    0.032936,\n",
      "           0.032785,    0.032634,    0.032483,    0.032332,    0.032181,     0.03203,    0.031878,    0.031727,    0.031576,    0.031425,    0.031274,    0.031123,    0.030972,    0.030821,     0.03067,    0.030519,    0.030368,    0.030217,    0.030065,    0.029914,    0.029763,    0.029612,    0.029461,\n",
      "            0.02931,    0.029159,    0.029008,    0.028857,    0.028706,    0.028555,    0.028404,    0.028253,    0.028101,     0.02795,    0.027799,    0.027648,    0.027497,    0.027346,    0.027195,    0.027044,    0.026893,    0.026742,    0.026591,     0.02644,    0.026288,    0.026137,    0.025986,\n",
      "           0.025835,    0.025684,    0.025533,    0.025382,    0.025231,     0.02508,    0.024929,    0.024778,    0.024627,    0.024475,    0.024324,    0.024173,    0.024022,    0.023871,     0.02372,    0.023569,    0.023418,    0.023267,    0.023116,    0.022965,    0.022814,    0.022662,    0.022511,\n",
      "            0.02236,    0.022209,    0.022058,    0.021907,    0.021756,    0.021605,    0.021454,    0.021303,    0.021152,    0.021001,    0.020849,    0.020698,    0.020547,    0.020396,    0.020245,    0.020094,    0.019943,    0.019792,    0.019641,     0.01949,    0.019339,    0.019188,    0.019036,\n",
      "           0.018885,    0.018734,    0.018583,    0.018432,    0.018281,     0.01813,    0.017979,    0.017828,    0.017677,    0.017526,    0.017375,    0.017223,    0.017072,    0.016921,     0.01677,    0.016619,    0.016468,    0.016317,    0.016166,    0.016015,    0.015864,    0.015713,    0.015562,\n",
      "            0.01541,    0.015259,    0.015108,    0.014957,    0.014806,    0.014655,    0.014504,    0.014353,    0.014202,    0.014051,      0.0139,    0.013749,    0.013597,    0.013446,    0.013295,    0.013144,    0.012993,    0.012842,    0.012691,     0.01254,    0.012389,    0.012238,    0.012087,\n",
      "           0.011936,    0.011784,    0.011633,    0.011482,    0.011331,     0.01118,    0.011029,    0.010878,    0.010727,    0.010576,    0.010425,    0.010274,    0.010123,   0.0099715,   0.0098204,   0.0096693,   0.0095182,   0.0093671,   0.0092161,    0.009065,   0.0089139,   0.0087628,   0.0086117,\n",
      "          0.0084606,   0.0083096,   0.0081585,   0.0080074,   0.0078563,   0.0077052,   0.0075541,   0.0074031,    0.007252,   0.0071009,   0.0069498,   0.0067987,   0.0066476,   0.0064966,   0.0063455,   0.0061944,   0.0060433,   0.0058922,   0.0057412,   0.0055901,    0.005439,   0.0052879,   0.0051368,\n",
      "          0.0049857,   0.0048347,   0.0046836,   0.0045325,   0.0043814,   0.0042303,   0.0040792,   0.0039282,   0.0037771,    0.003626,   0.0034749,   0.0033238,   0.0031727,   0.0030217,   0.0028706,   0.0027195,   0.0025684,   0.0024173,   0.0022662,   0.0021152,   0.0019641,    0.001813,   0.0016619,\n",
      "          0.0015108,   0.0013597,   0.0012087,   0.0010576,   0.0009065,  0.00075541,  0.00060433,  0.00045325,  0.00030217,  0.00015108,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.11307,     0.11307,     0.15242,     0.17419,     0.19709,     0.21534,     0.22067,     0.23412,     0.24659,     0.25165,     0.25165,      0.2571,     0.26834,     0.27501,     0.27979,     0.28365,     0.28323,     0.28583,     0.28832,      0.2914,     0.29554,     0.30051,     0.30326,\n",
      "             0.3004,     0.30254,     0.30616,     0.30676,     0.30897,     0.31125,     0.31329,     0.31415,     0.31506,     0.31642,     0.31794,     0.31884,     0.31989,     0.31557,     0.31971,     0.32225,     0.32634,     0.32885,     0.31904,     0.31893,     0.32031,     0.32474,     0.32569,\n",
      "            0.32663,     0.32762,     0.33042,     0.33355,     0.33455,     0.33852,     0.34001,     0.34153,     0.33512,     0.33669,     0.33724,      0.3378,     0.34095,     0.34185,     0.34225,     0.33988,     0.33752,     0.33515,     0.33583,      0.3371,     0.33841,     0.33935,      0.3386,\n",
      "            0.33716,     0.33572,     0.33429,     0.33285,     0.33191,     0.33256,      0.3332,     0.33334,     0.33037,     0.32739,     0.32768,     0.32857,     0.32942,     0.33034,      0.3316,     0.33194,     0.33219,     0.33244,     0.33269,     0.33295,      0.3332,     0.33377,     0.33473,\n",
      "            0.33684,      0.3372,     0.33756,     0.33791,     0.33827,     0.33946,     0.34058,     0.34159,     0.34347,     0.34426,     0.34505,     0.34546,     0.34579,     0.34612,     0.34644,     0.34677,     0.34793,     0.35013,     0.35115,     0.35192,     0.35267,     0.35341,     0.35415,\n",
      "            0.34605,     0.34703,     0.34763,     0.34803,     0.34843,     0.34882,     0.34933,     0.35284,      0.3532,     0.35347,     0.35374,     0.35401,     0.35428,     0.35455,     0.35481,     0.35495,     0.35506,     0.35518,      0.3553,     0.35542,     0.35553,     0.35565,     0.35577,\n",
      "            0.35589,       0.356,     0.35612,     0.35624,     0.35635,     0.35647,     0.35659,      0.3567,     0.35883,        0.36,     0.36087,     0.36138,     0.36188,     0.36238,     0.36557,       0.368,     0.36796,     0.36652,     0.36507,     0.36363,     0.36219,     0.36074,     0.35962,\n",
      "            0.35999,     0.36037,     0.36075,     0.36112,     0.36149,     0.36199,     0.36253,     0.36307,      0.3636,      0.3639,     0.36418,     0.36447,     0.36475,     0.36503,     0.36531,     0.36559,     0.36788,     0.36878,     0.36968,     0.37057,     0.37145,     0.37233,      0.3732,\n",
      "            0.37407,     0.37467,     0.37519,      0.3757,     0.37622,      0.3775,     0.38108,      0.3815,     0.38191,     0.38232,     0.38274,     0.38315,     0.38367,     0.38423,     0.38478,     0.38534,     0.38605,     0.38687,     0.38768,     0.38844,     0.38918,     0.38992,     0.39089,\n",
      "            0.39203,     0.39298,     0.39371,     0.39445,     0.39666,     0.39852,     0.39962,     0.40106,     0.40311,     0.40521,     0.40544,     0.40567,      0.4059,     0.40612,     0.40635,     0.40658,      0.4068,     0.40703,     0.40725,     0.40748,     0.40772,       0.408,     0.40827,\n",
      "            0.40855,     0.40882,     0.40909,     0.40937,     0.40964,     0.40991,     0.41019,      0.4118,     0.41356,     0.41496,     0.41784,     0.41695,     0.41606,     0.41517,     0.41428,     0.41339,     0.41249,      0.4116,     0.41071,     0.40981,     0.40892,     0.40802,       0.408,\n",
      "            0.40812,     0.40824,     0.40836,     0.40848,      0.4086,     0.40872,     0.40884,     0.40896,     0.40908,      0.4092,     0.40932,     0.40944,     0.40956,     0.40968,      0.4098,     0.40992,     0.41004,     0.41016,     0.41028,      0.4104,     0.41052,     0.41071,     0.41106,\n",
      "             0.4114,     0.41175,     0.41209,     0.41243,     0.41277,     0.41311,     0.41346,     0.41382,     0.41418,     0.41454,     0.41489,     0.41525,     0.41561,     0.41596,      0.4166,     0.41745,     0.41829,     0.41879,     0.41828,     0.41776,     0.41725,     0.41673,     0.41622,\n",
      "             0.4157,     0.41519,     0.41467,     0.41416,     0.41364,     0.41313,     0.41261,      0.4121,     0.41158,     0.41106,     0.41055,     0.41003,     0.40951,       0.409,     0.40848,      0.4083,     0.40864,     0.40899,     0.40933,     0.40968,     0.41002,     0.41036,     0.41071,\n",
      "            0.41102,     0.41126,     0.41151,     0.41175,     0.41199,     0.41223,     0.41247,     0.41271,     0.41295,     0.41319,     0.41343,     0.41367,     0.41672,     0.41697,     0.41721,     0.41745,     0.41769,     0.41793,     0.41817,     0.41841,     0.41865,     0.41889,     0.41912,\n",
      "            0.41936,      0.4197,     0.42116,     0.42254,     0.42261,     0.42268,     0.42275,     0.42283,      0.4229,     0.42297,     0.42304,     0.42311,     0.42318,     0.42325,     0.42332,      0.4234,     0.42347,     0.42354,     0.42361,     0.42368,     0.42375,     0.42382,     0.42389,\n",
      "            0.42396,     0.42403,     0.42411,     0.42418,     0.42425,     0.42432,     0.42439,     0.42446,     0.42453,      0.4246,     0.42467,     0.42474,     0.42481,     0.42488,     0.42495,     0.42502,     0.42509,     0.42516,     0.42523,      0.4253,     0.42537,     0.42545,     0.42552,\n",
      "            0.42621,     0.42709,     0.42797,     0.42885,     0.42976,     0.43067,     0.43157,     0.43422,     0.42807,     0.42381,     0.42483,     0.42584,     0.42654,     0.42673,     0.42692,      0.4271,     0.42729,     0.42748,     0.42766,     0.42785,     0.42804,     0.42822,     0.42841,\n",
      "            0.42859,     0.42878,     0.42896,     0.42914,     0.42933,     0.42951,     0.42977,     0.43017,     0.43056,     0.43096,     0.43135,     0.43174,     0.43213,     0.43252,     0.43295,     0.43355,     0.43416,     0.43475,     0.43535,     0.43594,     0.43272,     0.42822,      0.4239,\n",
      "            0.42099,     0.41808,     0.41517,     0.41225,      0.4107,     0.40916,     0.40763,     0.40609,     0.40455,     0.40301,     0.40147,     0.40004,     0.40096,     0.40188,     0.40278,     0.40374,     0.40473,      0.4057,      0.4015,     0.39394,     0.39477,     0.39559,      0.3964,\n",
      "            0.39701,     0.39741,      0.3978,     0.39819,     0.39858,     0.39897,     0.39935,     0.39974,     0.39935,     0.39728,     0.39522,     0.39315,     0.39108,       0.389,     0.38711,     0.38727,     0.38742,     0.38758,     0.38773,     0.38789,     0.38804,      0.3882,     0.38835,\n",
      "             0.3885,     0.38866,     0.38881,     0.38897,     0.38912,     0.38927,     0.38942,     0.38958,     0.38973,     0.38988,     0.39003,     0.39018,     0.39031,     0.39042,     0.39053,     0.39064,     0.39075,     0.39086,     0.39097,     0.39108,     0.39119,      0.3913,     0.39141,\n",
      "            0.39152,     0.39163,     0.39174,     0.39185,     0.39196,     0.39207,     0.39217,     0.39228,     0.39239,      0.3925,     0.39261,     0.39272,     0.39282,     0.39293,     0.39304,     0.39315,     0.39325,     0.39336,     0.39353,     0.39387,     0.39422,     0.39456,      0.3949,\n",
      "            0.39524,     0.39558,     0.39591,     0.39625,     0.39659,     0.39361,     0.38901,      0.3844,     0.38394,     0.38473,     0.38551,     0.38628,     0.38618,      0.3856,     0.38502,     0.38444,     0.38386,     0.38328,      0.3827,     0.38212,     0.38154,     0.38096,     0.38038,\n",
      "            0.37979,     0.37921,     0.37863,     0.37805,     0.37747,     0.37688,      0.3763,     0.37572,     0.37513,     0.37455,     0.37396,     0.37338,      0.3711,     0.35902,     0.35912,     0.35926,     0.35941,     0.35955,      0.3597,     0.35984,     0.35998,     0.36013,     0.36027,\n",
      "            0.36041,     0.36055,     0.36069,     0.36084,     0.36098,     0.36112,     0.36126,      0.3614,     0.36154,     0.36168,     0.36182,     0.36196,      0.3621,     0.36221,     0.36233,     0.36245,     0.36257,     0.36269,     0.36281,     0.36293,     0.36305,     0.36316,     0.36328,\n",
      "             0.3634,     0.36352,     0.36363,     0.36375,     0.36387,     0.36398,      0.3641,     0.36422,     0.36433,     0.36445,     0.36456,     0.36468,     0.36479,     0.36491,     0.36503,     0.36514,     0.36559,     0.36672,     0.36784,       0.368,      0.3671,      0.3662,      0.3653,\n",
      "             0.3644,      0.3635,      0.3626,      0.3617,      0.3608,      0.3599,     0.35899,     0.35809,     0.35718,     0.35628,     0.35537,     0.35447,      0.3541,     0.35437,     0.35463,     0.35489,     0.35515,      0.3554,     0.35566,     0.35592,     0.35617,     0.35642,     0.35668,\n",
      "            0.35693,     0.35692,     0.35539,     0.35386,     0.35232,     0.35079,     0.34925,     0.34771,     0.34617,     0.34463,     0.34309,     0.34152,     0.33993,     0.33834,     0.33674,     0.33515,     0.33355,     0.33195,     0.33035,     0.32874,     0.32728,     0.32742,     0.32756,\n",
      "             0.3277,     0.32784,     0.32798,     0.32812,     0.32826,      0.3284,     0.32854,     0.32867,     0.32881,     0.32895,     0.32908,     0.32922,     0.32935,     0.32949,     0.32962,     0.32976,     0.32989,     0.33003,     0.33016,     0.33031,     0.33058,     0.33084,     0.33111,\n",
      "            0.33137,     0.33163,     0.33189,     0.33215,     0.33241,     0.33266,     0.33292,     0.33317,     0.32457,     0.31278,     0.30585,     0.30203,     0.30237,     0.30271,     0.30304,     0.30337,      0.3037,     0.30403,     0.30435,     0.30468,     0.30491,     0.30511,     0.30531,\n",
      "            0.30551,      0.3057,      0.3059,      0.3061,     0.30629,     0.30649,     0.30668,     0.30687,     0.30706,     0.30725,     0.30744,     0.30763,     0.30718,     0.30641,     0.30565,     0.30488,     0.30412,     0.30335,     0.30258,     0.30181,     0.30105,     0.30028,     0.29951,\n",
      "            0.29874,     0.29797,      0.2972,     0.29643,     0.29566,     0.29489,     0.29411,     0.29334,     0.29257,     0.29179,     0.29134,      0.2916,     0.29186,     0.29212,     0.29237,     0.29263,     0.29288,     0.29313,     0.29338,     0.29362,     0.29387,     0.29411,     0.29427,\n",
      "            0.29442,     0.29457,     0.29472,     0.29487,     0.29502,     0.29517,     0.29532,     0.29546,     0.29561,     0.29576,      0.2959,     0.29605,     0.29619,     0.29634,     0.29648,     0.29662,     0.29677,     0.29691,     0.29707,     0.29735,     0.29763,     0.29791,     0.29819,\n",
      "            0.29846,     0.29873,       0.299,     0.29927,     0.29953,     0.29979,     0.30019,     0.30108,     0.30194,     0.30278,     0.30025,     0.29623,      0.2922,     0.28815,     0.28184,     0.27213,     0.26731,     0.26606,     0.26481,     0.26356,      0.2623,     0.26104,     0.25979,\n",
      "            0.25853,     0.25726,       0.256,     0.25474,     0.25347,      0.2522,     0.25093,     0.24888,     0.24465,     0.24041,     0.23615,     0.23187,     0.23279,       0.234,      0.2304,     0.22661,     0.22281,       0.219,     0.21517,     0.21308,     0.21105,       0.209,     0.20696,\n",
      "            0.20491,     0.20286,      0.2008,     0.19874,     0.19667,     0.13708,     0.13217,     0.12941,     0.12665,     0.12388,     0.12111,     0.11833,     0.11554,     0.11278,     0.11011,     0.10743,     0.10474,     0.10205,    0.099351,    0.096646,    0.093934,    0.087005,    0.069618,\n",
      "           0.067264,    0.064907,    0.062546,    0.060182,    0.057813,    0.055441,    0.053064,    0.050683,    0.048297,    0.037533,    0.023813,    0.023825,    0.023836,    0.023847,    0.023857,    0.023867,    0.023877,    0.023886,    0.023895,    0.023904,    0.023912,     0.02392,    0.023928,\n",
      "           0.023935,    0.023942,     0.02395,    0.023956,    0.023963,    0.023969,    0.023976,    0.023982,    0.023988,    0.023993,    0.023999,    0.024004,    0.024009,    0.024015,     0.02402,    0.024024,    0.024029,    0.024034,    0.024038,    0.024043,    0.024047,    0.024051,    0.024055,\n",
      "           0.024059,    0.024063,    0.024067,     0.02407,    0.024074,    0.024078,    0.024081,    0.024084,    0.024088,    0.024091,    0.024094,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.062581,    0.062581,    0.087896,      0.1031,     0.11955,     0.13325,      0.1381,      0.1488,     0.15902,     0.16437,     0.16555,      0.1703,      0.1803,     0.18637,     0.19079,      0.1944,     0.19573,     0.19822,     0.20063,     0.20362,     0.20768,     0.21263,      0.2154,\n",
      "            0.21469,     0.21689,     0.22063,     0.22126,     0.22356,     0.22595,     0.22811,     0.22903,        0.23,     0.23144,     0.23307,     0.23404,     0.23517,     0.23322,     0.23777,      0.2406,     0.24518,     0.24802,     0.24213,     0.24312,     0.24474,     0.24995,     0.25108,\n",
      "            0.25219,     0.25337,     0.25674,     0.26054,     0.26176,     0.26666,      0.2685,     0.27041,     0.26654,     0.26853,     0.26923,     0.26994,     0.27399,     0.27516,     0.27588,      0.2742,     0.27253,     0.27086,     0.27191,     0.27358,     0.27532,     0.27657,     0.27619,\n",
      "            0.27517,     0.27414,     0.27312,      0.2721,     0.27153,     0.27239,     0.27326,     0.27445,     0.27231,     0.27018,     0.27087,     0.27209,     0.27326,     0.27453,     0.27627,     0.27674,     0.27709,     0.27744,      0.2778,     0.27815,      0.2785,      0.2793,     0.28064,\n",
      "            0.28363,     0.28414,     0.28464,     0.28515,     0.28566,     0.28736,     0.28898,     0.29042,     0.29315,     0.29431,     0.29547,     0.29606,     0.29655,     0.29703,     0.29752,       0.298,     0.29971,       0.303,     0.30453,     0.30568,     0.30682,     0.30794,     0.30907,\n",
      "            0.30352,     0.30504,     0.30596,     0.30658,      0.3072,     0.30781,      0.3086,     0.31413,      0.3147,     0.31513,     0.31556,     0.31598,     0.31641,     0.31684,     0.31727,     0.31748,     0.31767,     0.31786,     0.31804,     0.31823,     0.31842,     0.31861,      0.3188,\n",
      "            0.31899,     0.31917,     0.31936,     0.31955,     0.31974,     0.31993,     0.32012,      0.3203,     0.32375,     0.32565,     0.32709,     0.32792,     0.32875,     0.32957,      0.3349,     0.33899,     0.33964,     0.33856,     0.33747,     0.33639,     0.33531,     0.33423,     0.33345,\n",
      "             0.3341,     0.33474,     0.33539,     0.33604,     0.33669,     0.33756,     0.33849,     0.33943,     0.34037,     0.34089,     0.34139,     0.34188,     0.34238,     0.34287,     0.34337,     0.34386,     0.34793,     0.34955,     0.35117,     0.35278,     0.35439,     0.35599,     0.35759,\n",
      "            0.35919,     0.36029,     0.36125,      0.3622,     0.36316,     0.36555,     0.37234,     0.37313,     0.37393,     0.37472,     0.37551,      0.3763,     0.37732,      0.3784,     0.37947,     0.38055,     0.38195,     0.38355,     0.38515,     0.38665,     0.38813,      0.3896,     0.39154,\n",
      "            0.39382,     0.39575,     0.39724,     0.39874,     0.40329,     0.40716,     0.40945,     0.41249,     0.41685,     0.42138,     0.42187,     0.42236,     0.42286,     0.42335,     0.42384,     0.42434,     0.42483,     0.42532,     0.42582,     0.42631,     0.42683,     0.42744,     0.42804,\n",
      "            0.42865,     0.42925,     0.42986,     0.43046,     0.43107,     0.43167,     0.43228,     0.43588,     0.43984,     0.44303,     0.45036,     0.44969,     0.44901,     0.44834,     0.44767,     0.44699,     0.44632,     0.44565,     0.44497,      0.4443,     0.44363,     0.44295,      0.4431,\n",
      "            0.44339,     0.44367,     0.44396,     0.44424,     0.44453,     0.44481,      0.4451,     0.44538,     0.44567,     0.44595,     0.44624,     0.44652,     0.44681,     0.44709,     0.44738,     0.44766,     0.44795,     0.44823,     0.44852,      0.4488,     0.44909,     0.44955,     0.45038,\n",
      "            0.45121,     0.45204,     0.45287,     0.45369,     0.45452,     0.45535,      0.4562,     0.45707,     0.45795,     0.45882,      0.4597,     0.46057,     0.46145,     0.46232,     0.46392,     0.46602,     0.46812,      0.4696,     0.46921,     0.46882,     0.46843,     0.46804,     0.46764,\n",
      "            0.46725,     0.46686,     0.46647,     0.46608,     0.46569,      0.4653,     0.46491,     0.46452,     0.46413,     0.46373,     0.46334,     0.46295,     0.46256,     0.46217,     0.46178,     0.46188,     0.46277,     0.46365,     0.46454,     0.46543,     0.46632,     0.46721,     0.46809,\n",
      "            0.46892,     0.46955,     0.47018,     0.47081,     0.47144,     0.47207,      0.4727,     0.47334,     0.47397,      0.4746,     0.47523,     0.47586,     0.48402,     0.48468,     0.48533,     0.48598,     0.48664,     0.48729,     0.48794,     0.48859,     0.48925,      0.4899,     0.49055,\n",
      "             0.4912,     0.49214,     0.49618,     0.50001,     0.50021,     0.50041,     0.50061,     0.50081,     0.50101,     0.50121,     0.50141,     0.50161,     0.50181,     0.50201,     0.50222,     0.50242,     0.50262,     0.50282,     0.50302,     0.50322,     0.50342,     0.50362,     0.50382,\n",
      "            0.50402,     0.50422,     0.50442,     0.50462,     0.50482,     0.50502,     0.50522,     0.50542,     0.50562,     0.50582,     0.50602,     0.50622,     0.50642,     0.50662,     0.50682,     0.50702,     0.50722,     0.50743,     0.50763,     0.50783,     0.50803,     0.50823,     0.50843,\n",
      "            0.51041,     0.51295,     0.51548,     0.51806,     0.52073,     0.52339,     0.52606,       0.534,     0.53075,     0.52868,     0.53187,     0.53506,     0.53726,     0.53786,     0.53845,     0.53905,     0.53964,     0.54024,     0.54084,     0.54143,     0.54203,     0.54262,     0.54322,\n",
      "            0.54381,     0.54441,       0.545,      0.5456,      0.5462,     0.54679,     0.54764,     0.54893,     0.55021,      0.5515,     0.55279,     0.55407,     0.55536,     0.55665,     0.55807,     0.56008,     0.56209,     0.56411,     0.56612,     0.56813,     0.56616,     0.56289,     0.55974,\n",
      "            0.55757,     0.55539,     0.55322,     0.55105,     0.54986,     0.54868,      0.5475,     0.54632,     0.54515,     0.54397,     0.54279,     0.54182,     0.54522,     0.54861,       0.552,     0.55562,     0.55936,      0.5631,     0.56155,     0.55651,     0.55982,     0.56314,     0.56646,\n",
      "            0.56896,     0.57058,      0.5722,     0.57381,     0.57543,     0.57705,     0.57867,     0.58029,     0.58089,     0.57929,     0.57769,     0.57609,     0.57449,      0.5729,     0.57148,     0.57216,     0.57285,     0.57353,     0.57421,     0.57489,     0.57557,     0.57625,     0.57693,\n",
      "            0.57761,     0.57829,     0.57897,     0.57965,     0.58033,     0.58101,     0.58169,     0.58237,     0.58305,     0.58374,     0.58442,      0.5851,     0.58567,     0.58617,     0.58666,     0.58716,     0.58766,     0.58816,     0.58866,     0.58916,     0.58966,     0.59015,     0.59065,\n",
      "            0.59115,     0.59165,     0.59215,     0.59265,     0.59314,     0.59364,     0.59414,     0.59464,     0.59514,     0.59564,     0.59614,     0.59663,     0.59713,     0.59763,     0.59813,     0.59863,     0.59913,     0.59963,      0.6004,     0.60201,     0.60361,     0.60522,     0.60683,\n",
      "            0.60843,     0.61004,     0.61165,     0.61325,     0.61486,     0.61304,     0.60955,     0.60607,     0.60831,     0.61227,     0.61624,      0.6202,     0.62133,     0.62089,     0.62044,     0.61999,     0.61955,      0.6191,     0.61865,      0.6182,     0.61776,     0.61731,     0.61686,\n",
      "            0.61641,     0.61597,     0.61552,     0.61507,     0.61462,     0.61418,     0.61373,     0.61328,     0.61283,     0.61239,     0.61194,     0.61149,     0.60969,     0.60004,     0.60081,     0.60162,     0.60243,     0.60324,     0.60405,     0.60486,     0.60567,     0.60648,     0.60729,\n",
      "             0.6081,     0.60891,     0.60972,     0.61053,     0.61134,     0.61215,     0.61296,     0.61377,     0.61459,      0.6154,     0.61621,     0.61702,      0.6178,      0.6185,     0.61919,     0.61989,     0.62059,     0.62128,     0.62198,     0.62267,     0.62337,     0.62406,     0.62476,\n",
      "            0.62546,     0.62615,     0.62685,     0.62754,     0.62824,     0.62894,     0.62963,     0.63033,     0.63102,     0.63172,     0.63241,     0.63311,     0.63381,      0.6345,      0.6352,     0.63589,     0.63863,      0.6456,     0.65257,     0.65592,     0.65523,     0.65454,     0.65385,\n",
      "            0.65315,     0.65246,     0.65177,     0.65107,     0.65038,     0.64969,       0.649,      0.6483,     0.64761,     0.64692,     0.64623,     0.64553,     0.64597,     0.64772,     0.64946,     0.65121,     0.65296,      0.6547,     0.65645,     0.65819,     0.65994,     0.66169,     0.66343,\n",
      "            0.66518,     0.66649,      0.6653,      0.6641,     0.66291,     0.66172,     0.66052,     0.65933,     0.65814,     0.65694,     0.65575,      0.6545,     0.65319,     0.65189,     0.65058,     0.64927,     0.64797,     0.64666,     0.64536,     0.64405,     0.64295,     0.64403,     0.64512,\n",
      "             0.6462,     0.64729,     0.64837,     0.64946,     0.65054,     0.65163,     0.65271,      0.6538,     0.65488,     0.65597,     0.65705,     0.65813,     0.65922,      0.6603,     0.66139,     0.66247,     0.66356,     0.66464,     0.66573,     0.66696,     0.66914,     0.67132,     0.67349,\n",
      "            0.67567,     0.67785,     0.68003,     0.68221,     0.68438,     0.68656,     0.68874,     0.69092,     0.68537,     0.67581,     0.66998,      0.6681,     0.67143,     0.67476,      0.6781,     0.68143,     0.68476,     0.68809,     0.69142,     0.69475,     0.69719,     0.69929,     0.70139,\n",
      "            0.70349,     0.70559,     0.70769,     0.70979,     0.71189,     0.71399,     0.71609,     0.71819,     0.72029,     0.72239,     0.72449,     0.72659,     0.72686,     0.72626,     0.72565,     0.72504,     0.72443,     0.72382,     0.72322,     0.72261,       0.722,     0.72139,     0.72078,\n",
      "            0.72018,     0.71957,     0.71896,     0.71835,     0.71774,     0.71714,     0.71653,     0.71592,     0.71531,      0.7147,     0.71527,     0.71842,     0.72157,     0.72472,     0.72787,     0.73102,     0.73416,     0.73731,     0.74046,     0.74361,     0.74676,     0.74991,     0.75193,\n",
      "            0.75392,     0.75591,      0.7579,     0.75989,     0.76188,     0.76387,     0.76586,     0.76785,     0.76984,     0.77183,     0.77382,     0.77581,      0.7778,     0.77979,     0.78178,     0.78377,     0.78576,     0.78775,     0.79001,     0.79403,     0.79804,     0.80206,     0.80608,\n",
      "            0.81009,     0.81411,     0.81813,     0.82214,     0.82616,     0.83018,     0.83631,     0.85025,     0.86418,     0.87811,     0.88116,     0.87945,     0.87774,     0.87603,     0.87316,     0.86858,     0.86628,     0.86561,     0.86495,     0.86428,     0.86362,     0.86295,     0.86229,\n",
      "            0.86162,     0.86096,     0.86029,     0.85963,     0.85896,      0.8583,     0.85763,     0.85647,     0.85393,      0.8514,     0.84886,     0.84632,     0.87963,     0.91549,      0.9152,     0.91368,     0.91217,     0.91065,     0.90914,     0.90816,      0.9072,     0.90624,     0.90528,\n",
      "            0.90432,     0.90336,     0.90239,     0.90143,     0.90047,     0.85907,     0.85413,     0.85101,      0.8479,     0.84479,     0.84168,     0.83856,     0.83545,     0.83201,     0.82787,     0.82373,     0.81959,     0.81544,      0.8113,     0.80716,     0.80302,     0.78877,     0.74945,\n",
      "            0.74075,     0.73205,     0.72336,     0.71466,     0.70596,     0.69726,     0.68857,     0.67987,     0.67117,     0.59811,      0.5033,     0.51393,     0.52455,     0.53518,     0.54581,     0.55644,     0.56706,     0.57769,     0.58832,     0.59895,     0.60957,      0.6202,     0.63083,\n",
      "            0.64146,     0.65208,     0.66271,     0.67334,     0.68397,      0.6946,     0.70522,     0.71585,     0.72648,     0.73711,     0.74773,     0.75836,     0.76899,     0.77962,     0.79024,     0.80087,      0.8115,     0.82213,     0.83275,     0.84338,     0.85401,     0.86464,     0.87526,\n",
      "            0.88589,     0.89652,     0.90715,     0.91777,      0.9284,     0.93903,     0.94966,     0.96029,     0.97091,     0.98154,     0.99217,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.58537,     0.58537,     0.57317,     0.56098,     0.56098,     0.56098,     0.54878,     0.54878,     0.54878,     0.53659,     0.52439,     0.52439,     0.52439,     0.52439,     0.52439,     0.52439,      0.5122,      0.5122,      0.5122,      0.5122,      0.5122,      0.5122,      0.5122,\n",
      "                0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,         0.5,      0.4878,      0.4878,      0.4878,      0.4878,      0.4878,     0.46755,     0.46341,     0.46341,     0.46341,     0.46341,\n",
      "            0.46341,     0.46341,     0.46341,     0.46341,     0.46341,     0.46341,     0.46341,     0.46341,     0.45122,     0.45122,     0.45122,     0.45122,     0.45122,     0.45122,     0.45067,     0.44693,     0.44319,     0.43944,     0.43902,     0.43902,     0.43902,     0.43902,     0.43744,\n",
      "            0.43521,     0.43298,     0.43075,     0.42852,     0.42683,     0.42683,     0.42683,     0.42441,     0.41988,     0.41535,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,\n",
      "            0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,     0.41463,\n",
      "            0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,\n",
      "            0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40244,     0.40143,     0.39951,     0.39759,     0.39567,     0.39375,     0.39183,     0.39024,\n",
      "            0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,\n",
      "            0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,\n",
      "            0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,\n",
      "            0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.39024,     0.38971,     0.38866,     0.38762,     0.38657,     0.38552,     0.38448,     0.38343,     0.38239,     0.38134,     0.38029,     0.37925,      0.3782,     0.37805,\n",
      "            0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,\n",
      "            0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,     0.37805,      0.3779,     0.37732,     0.37673,     0.37615,     0.37557,     0.37498,\n",
      "             0.3744,     0.37381,     0.37323,     0.37264,     0.37206,     0.37147,     0.37089,     0.37031,     0.36972,     0.36914,     0.36855,     0.36797,     0.36738,      0.3668,     0.36621,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,\n",
      "            0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,\n",
      "            0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,\n",
      "            0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,\n",
      "            0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.36585,     0.35868,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,\n",
      "            0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35366,     0.35018,     0.34555,     0.34111,\n",
      "            0.33816,     0.33521,     0.33226,      0.3293,     0.32775,     0.32622,     0.32468,     0.32314,     0.32161,     0.32007,     0.31854,     0.31707,     0.31707,     0.31707,     0.31707,     0.31707,     0.31707,     0.31707,     0.31245,     0.30488,     0.30488,     0.30488,     0.30488,\n",
      "            0.30488,     0.30488,     0.30488,     0.30488,     0.30488,     0.30488,     0.30488,     0.30488,     0.30426,      0.3023,     0.30035,     0.29839,     0.29643,     0.29448,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,\n",
      "            0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,\n",
      "            0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.29268,\n",
      "            0.29268,     0.29268,     0.29268,     0.29268,     0.29268,     0.28986,     0.28566,     0.28146,     0.28049,     0.28049,     0.28049,     0.28049,     0.28016,     0.27964,     0.27912,      0.2786,     0.27808,     0.27756,     0.27704,     0.27652,       0.276,     0.27548,     0.27496,\n",
      "            0.27445,     0.27393,     0.27341,     0.27289,     0.27237,     0.27185,     0.27133,     0.27081,     0.27029,     0.26977,     0.26925,     0.26874,     0.26673,     0.25614,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,\n",
      "             0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,\n",
      "             0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,      0.2561,     0.25574,     0.25498,     0.25421,     0.25345,\n",
      "            0.25269,     0.25193,     0.25117,     0.25041,     0.24964,     0.24888,     0.24812,     0.24736,      0.2466,     0.24583,     0.24507,     0.24431,      0.2439,      0.2439,      0.2439,      0.2439,      0.2439,      0.2439,      0.2439,      0.2439,      0.2439,      0.2439,      0.2439,\n",
      "             0.2439,     0.24372,     0.24245,     0.24118,     0.23992,     0.23865,     0.23738,     0.23612,     0.23485,     0.23358,     0.23232,     0.23104,     0.22974,     0.22845,     0.22716,     0.22587,     0.22457,     0.22328,     0.22199,      0.2207,     0.21951,     0.21951,     0.21951,\n",
      "            0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,\n",
      "            0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21951,     0.21263,     0.20348,     0.19816,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,\n",
      "            0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19512,     0.19474,     0.19417,      0.1936,     0.19302,     0.19245,     0.19188,     0.19131,     0.19074,     0.19017,      0.1896,     0.18903,\n",
      "            0.18846,     0.18789,     0.18732,     0.18674,     0.18617,      0.1856,     0.18503,     0.18446,     0.18389,     0.18332,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,\n",
      "            0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,\n",
      "            0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18293,     0.18095,     0.17811,     0.17527,     0.17243,     0.16804,     0.16134,     0.15804,     0.15719,     0.15634,     0.15549,     0.15463,     0.15378,     0.15293,\n",
      "            0.15208,     0.15123,     0.15037,     0.14952,     0.14867,     0.14782,     0.14697,     0.14559,     0.14278,     0.13996,     0.13715,     0.13433,     0.13415,     0.13415,     0.13179,     0.12935,     0.12691,     0.12446,     0.12202,      0.1207,     0.11941,     0.11812,     0.11683,\n",
      "            0.11555,     0.11426,     0.11297,     0.11168,     0.11039,    0.074485,    0.071626,    0.070032,    0.068438,    0.066843,    0.065249,    0.063655,    0.062061,    0.060492,    0.058976,    0.057461,    0.055946,    0.054431,    0.052915,      0.0514,    0.049885,    0.046042,    0.036505,\n",
      "           0.035232,    0.033959,    0.032686,    0.031413,    0.030141,    0.028868,    0.027595,    0.026322,     0.02505,    0.019374,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,\n",
      "           0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,\n",
      "           0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,    0.012195,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
      "fitness: np.float64(0.17336699850808598)\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
      "maps: array([    0.15378])\n",
      "names: {0: 'fail'}\n",
      "plot: True\n",
      "results_dict: {'metrics/precision(B)': np.float64(0.5080262470439122), 'metrics/recall(B)': np.float64(0.36585365853658536), 'metrics/mAP50(B)': np.float64(0.34968525924811467), 'metrics/mAP50-95(B)': np.float64(0.1537760806480828), 'fitness': np.float64(0.17336699850808598)}\n",
      "save_dir: WindowsPath('d:/Coding Projects/DefectEye/runs/detect/defecteye_finetuned2')\n",
      "speed: {'preprocess': 0.3965844710667928, 'inference': 11.190662781397501, 'loss': 0.0, 'postprocess': 0.2547800540924072}\n",
      "task: 'detect'\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance on validation set\n",
    "metrics = model.val()\n",
    "print(metrics)  # Print evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Testing the Fine-Tuned Model on sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\Coding Projects\\DefectEye\\model-Test\\img1.jpg: 384x640 1 fail, 43.7ms\n",
      "Speed: 0.0ms preprocess, 43.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1md:\\Coding Projects\\DefectEye\\runs\\detect\\predict\u001b[0m\n",
      "\n",
      "image 1/1 D:\\Coding Projects\\DefectEye\\model-Test\\img3.jpg: 448x640 (no detections), 36.5ms\n",
      "Speed: 2.0ms preprocess, 36.5ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1md:\\Coding Projects\\DefectEye\\runs\\detect\\predict\u001b[0m\n",
      "\n",
      "image 1/1 D:\\Coding Projects\\DefectEye\\model-Test\\img4.jpg: 640x448 4 fails, 35.0ms\n",
      "Speed: 2.0ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1md:\\Coding Projects\\DefectEye\\runs\\detect\\predict\u001b[0m\n",
      "\n",
      "image 1/1 D:\\Coding Projects\\DefectEye\\model-Test\\img5.jpg: 480x640 (no detections), 47.5ms\n",
      "Speed: 1.0ms preprocess, 47.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1md:\\Coding Projects\\DefectEye\\runs\\detect\\predict\u001b[0m\n",
      "\n",
      "image 1/1 D:\\Coding Projects\\DefectEye\\model-Test\\img6.jpg: 480x640 (no detections), 38.5ms\n",
      "Speed: 1.0ms preprocess, 38.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1md:\\Coding Projects\\DefectEye\\runs\\detect\\predict\u001b[0m\n",
      "Predictions completed. Check the YOLO output folder: 'runs/detect/predict/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load trained YOLOv8 model\n",
    "model = YOLO('runs/detect/defecteye_finetuned/weights/best.pt')  \n",
    "\n",
    "# Define test images directory\n",
    "test_images_path = \"D:/Coding Projects/DefectEye/model-Test\"\n",
    "\n",
    "# Run YOLO predictions on all test images in the folder\n",
    "for img_name in os.listdir(test_images_path):\n",
    "    if img_name.endswith(('.jpg', '.png')):  # Process only images\n",
    "        img_path = os.path.join(test_images_path, img_name)\n",
    "        \n",
    "        # Run prediction and save results in YOLO output folder (default: runs/detect/predict/)\n",
    "        model.predict(source=img_path, save=True, imgsz=640)\n",
    "\n",
    "print(\"Predictions completed. Check the YOLO output folder: 'runs/detect/predict/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model as TorchScript for Raspberry Pi AI Hat\n",
    "model.export(format='torchscript', imgsz=640, optimize=True)\n",
    "\n",
    "# ONNX version \n",
    "model.export(format='onnx', imgsz=640, optimize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
